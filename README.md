# ğŸ” Probabilistic Sequence Models

This project demonstrates sequence modeling using **Hidden Markov Models (HMMs)** and the **Viterbi algorithm**, a foundational method for inferring hidden state sequences based on observed data.

These methods are widely used in NLP tasks like part-of-speech tagging, named entity recognition, and speech recognition.

---

## ğŸ““ Notebook Included

| Notebook | Focus | Description |
|----------|-------|-------------|
| `Hidden_Markov_Model_Viterbi.ipynb` | ğŸ” HMM & Viterbi Decoding | Models hidden state sequences using HMMs and applies the Viterbi algorithm to decode the most likely state path given a sequence of observations. |

---

## ğŸ” Key Concepts

- Hidden Markov Models (HMMs)
- Viterbi Decoding Algorithm
- Transition and Emission Probabilities
- Sequence Tagging & Inference
- Applications in NLP and Time-Series

---

## ğŸ›  Technologies Used

- Python
- NumPy
- Jupyter Notebook

---

## ğŸ“ Repo Structure

probabilistic-sequence-models/  
â”œâ”€â”€ Hidden_Markov_Model_Viterbi.ipynb  
â””â”€â”€ README.md

---

## â­ Purpose

This project illustrates how probabilistic models can be used for sequence prediction by leveraging the structure of Hidden Markov Models and the Viterbi algorithm. It focuses on the logic and mechanics behind these widely-used approaches.
